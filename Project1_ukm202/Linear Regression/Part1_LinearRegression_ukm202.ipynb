{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Part1_LinearRegression_pseudo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x__M5ogVVEHO"
      },
      "source": [
        "# Simple Linear versus Ridge Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9V6-6TJVEHW"
      },
      "source": [
        "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
        "\n",
        "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_PCl4yaVEHX"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.linear_model\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO0F_F2rVEHY"
      },
      "source": [
        "###  Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghnfPHU_VEHY"
      },
      "source": [
        "# Import the boston dataset from sklearn\n",
        "# Load dataset to some variable \n",
        "boston_data = load_boston()\n",
        "# print(boston_data)\n",
        "# boston_data.DESCR"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPAl4egxVEHZ",
        "outputId": "04bee447-5be9-483d-e45c-371ea10acdd4"
      },
      "source": [
        "#  Create X and Y variables - X holding the .data and Y holding .target \n",
        "X = boston_data.data\n",
        "y = boston_data.target\n",
        "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
        "# print(y.shape)\n",
        "# print(X.shape)\n",
        "# y.reshape(253,2)\n",
        "y.reshape(506,1)\n",
        "# print(\"Rank\", np.linalg.matrix_rank(y))\n",
        "\n",
        "# Observe the number of features and the number of labels\n",
        "print('The number of features is: ', X.shape[1])\n",
        "# Printing out the features\n",
        "print('The features: ', boston_data.feature_names)\n",
        "# The number of examples\n",
        "print('The number of exampels in our dataset: ', X.shape[0])\n",
        "# Observing the first 2 rows of the data\n",
        "print(X[0:2])\n",
        "print(y[0:2])\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features is:  13\n",
            "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "The number of exampels in our dataset:  506\n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00]]\n",
            "[24.  21.6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2hAQP53VEHZ"
      },
      "source": [
        "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM5zO7t4VEHZ"
      },
      "source": [
        "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
        "poly = PolynomialFeatures(2)\n",
        "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
        "# Simply copy Y into Y_2 \n",
        "\n",
        "X_2 = poly.fit_transform(X)\n",
        "y_2 = y\n",
        "# print(X_2)\n",
        "# print(\"======================================\")\n",
        "# print(y_2)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey6LU5LFVEHb",
        "outputId": "61dfd848-02c6-4035-d70e-985396c60a84"
      },
      "source": [
        "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 105)\n",
            "(506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-uoQA6NVEHb"
      },
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAuUifODVEHc"
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
        "  i,j = X_train.shape\n",
        "  I = np.identity(j)\n",
        "  X_train_trans_dot_X_train = np.dot(np.transpose(X_train),X_train)\n",
        "  alpha_times_I = alpha*I\n",
        "  X_train_trans_dot_y_train = np.dot(np.transpose(X_train),y_train)\n",
        "  M = X_train_trans_dot_X_train + alpha_times_I\n",
        "  M_Inverse = np.linalg.pinv(M)\n",
        "\n",
        "  w = np.dot(M_Inverse, X_train_trans_dot_y_train)\n",
        "  return w\n",
        "\n"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEXkg7dxVEHc"
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "def get_coeff_linear_normaleq(X_train, y_train):\n",
        "  X_train_trans_dot_X_train = np.dot(np.transpose(X_train),X_train)\n",
        "  X_train_trans_dot_y_train = np.dot(np.transpose(X_train),y_train)\n",
        "  M = X_train_trans_dot_X_train\n",
        "  M_Inverse = np.linalg.pinv(M)\n",
        "\n",
        "  w = np.dot(M_Inverse, X_train_trans_dot_y_train)\n",
        "  return w\n",
        "  "
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-k3eseyVEHc"
      },
      "source": [
        "# Define the evaluate_err_ridge function.\n",
        "# Return the train_error and test_error values\n",
        "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
        "    # pred_train=prediction using w and X_train+np.mean(y_train)\n",
        "    pred_train= np.dot(X_train, w)\n",
        "   \n",
        "#     pred_test=prediction using w and X_test\n",
        "    pred_test= np.dot(X_test, w)\n",
        "\n",
        "#     remember to add the mean back\n",
        "    # M = (y_train - pred_train)**2\n",
        "    M = np.square(y_train - pred_train)\n",
        "\n",
        "    train_error= np.mean(M)\n",
        "    N = np.square(y_test - pred_test)\n",
        "    test_error= np.mean(N)\n",
        "    \n",
        "    return train_error, test_error"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQaQb4jtVEHd"
      },
      "source": [
        "# Finish writting the k_fold_cross_validation function. \n",
        "# Returns the average training error and average test error from the k-fold cross validation\n",
        "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "# train_error_list = []\n",
        "# test_error_list =[]\n",
        "def k_fold_cross_validation_for_RR(k, X, y, alpha):\n",
        "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
        "    total_E_val_test = 0\n",
        "    total_E_val_train = 0\n",
        "    train_error_list = []\n",
        "    test_error_list =[]\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
        "        # Subtract y_train_mean from y_train and y_test\n",
        "        y_train_mean = np.mean(y_train)\n",
        "        y_train = y_train - y_train_mean\n",
        "        y_test = y_test - y_train_mean\n",
        "        \n",
        "        # Scaling the data matrix\n",
        "        scaler= sklearn.preprocessing.StandardScaler().fit(X_train, y_train)\n",
        "\n",
        "        # And scaler.transform(...)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        \n",
        "        # Determine the training error and the test error\n",
        "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
        "\n",
        "        # w = get_coeff_linear_normaleq(X_train, y_train)\n",
        "        w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
        "        # And use evaluate_err()\n",
        "        total_E_val_test, total_E_val_train = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
        "        # print(\"---------------------- \", alpha,\"----------------------\")\n",
        "        # print(\"Test Error \", total_E_val_test, \"Train Error  \", total_E_val_train)\n",
        "\n",
        "        train_error_list.append(total_E_val_train)\n",
        "        test_error_list.append(total_E_val_test)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        ##############\n",
        "    print(\"Average train error: \", np.mean(train_error_list))\n",
        "    print(\"Average test error : \", np.mean(test_error_list))\n",
        "    return  total_E_val_test, total_E_val_train\n",
        "    \n"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUUE8FHJi2PY"
      },
      "source": [
        "def k_fold_cross_validation_for_LR(k, X, y):\n",
        "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
        "    total_E_val_test = 0\n",
        "    total_E_val_train = 0\n",
        "    train_error_list = []\n",
        "    test_error_list =[]\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
        "        # Subtract y_train_mean from y_train and y_test\n",
        "        y_train_mean = np.mean(y_train)\n",
        "        y_train = y_train - y_train_mean\n",
        "        y_test = y_test - y_train_mean\n",
        "        \n",
        "        # Scaling the data matrix\n",
        "        scaler= sklearn.preprocessing.StandardScaler().fit(X_train, y_train)\n",
        "\n",
        "        # And scaler.transform(...)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        \n",
        "        # Determine the training error and the test error\n",
        "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
        "\n",
        "        w = get_coeff_linear_normaleq(X_train, y_train)\n",
        "        # And use evaluate_err()\n",
        "        total_E_val_test, total_E_val_train = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
        "        # print(\"---------------------- \", alpha,\"----------------------\")\n",
        "        # print(\"Test Error \", total_E_val_test, \"Train Error  \", total_E_val_train)\n",
        "\n",
        "        train_error_list.append(total_E_val_train)\n",
        "        test_error_list.append(total_E_val_test)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        ##############\n",
        "    print(\"Average train error: \", np.mean(train_error_list))\n",
        "    print(\"Average test error : \", np.mean(test_error_list))\n",
        "    return  total_E_val_test, total_E_val_train"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptucqZp5VEHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69ebb1a-042a-4fd6-d823-c7c9e8e7b103"
      },
      "source": [
        "# print the error for the both linear regression and ridge regression\n",
        "\n",
        "# the error should include both training error and testing error\n",
        "\n",
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"============== Error for Ridge Regression  ==========================\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "test, train = k_fold_cross_validation_for_RR(13, X, y, 10)\n",
        "print(\"Test Error \", test, \"Train Error  \", train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"================== Using PolynomialFeatures(degree=2) ======================\")\n",
        "print(\"\\n\")\n",
        "\n",
        "test, train = k_fold_cross_validation_for_RR(13, X_2, y_2, 10)\n",
        "print(\"Test Error \", test, \"Train Error  \", train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============== Error for Ridge Regression  ==========================\n",
            "\n",
            "\n",
            "Average train error:  23.744830717361012\n",
            "Average test error :  21.909107606951896\n",
            "Test Error  20.257047684784677 Train Error   44.37520576905882\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================== Using PolynomialFeatures(degree=2) ======================\n",
            "\n",
            "\n",
            "Average train error:  13.421591299161484\n",
            "Average test error :  10.044478093371858\n",
            "Test Error  9.203691183870086 Train Error   29.748698809980397\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H3OkjdOFIGm",
        "outputId": "b0399f72-f389-45a9-8d38-751a2948cf9a"
      },
      "source": [
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"============== Error for Linear Regression  ==========================\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "test, train = k_fold_cross_validation_for_LR(13, X, y)\n",
        "print(\"Test Error \", test, \"Train Error  \", train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"================== Using PolynomialFeatures(degree=2) ======================\")\n",
        "print(\"\\n\")\n",
        "\n",
        "test, train = k_fold_cross_validation_for_LR(13, X_2, y_2)\n",
        "print(\"Test Error \", test, \"Train Error  \", train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============== Error for Linear Regression  ==========================\n",
            "\n",
            "\n",
            "Average train error:  23.71387887738641\n",
            "Average test error :  21.82585480837458\n",
            "Test Error  20.17690678000032 Train Error   43.710386884038705\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================== Using PolynomialFeatures(degree=2) ======================\n",
            "\n",
            "\n",
            "Average train error:  12.3890617366037\n",
            "Average test error :  5.844996839595582\n",
            "Test Error  5.557537678823688 Train Error   17.9763604203099\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apBGWCLPVEHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ffef28-6609-48b3-e9a8-ebd8a5f101b3"
      },
      "source": [
        "# use the model to predict the new test case.\n",
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"===================== Error for Ridge Regression ===============================\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "for i in np.logspace(1,7,num=13):\n",
        "  print(\"---------------------- \", i,\"----------------------\")\n",
        "  \n",
        "  test, train = k_fold_cross_validation_for_RR(13, X, y, i)\n",
        "  print(\"Test Error \", test, \"Train Error  \", train)\n",
        "  print(\"\\n\")\n",
        "# print(\"Average train error: \", np.mean(train_error_list))\n",
        "# print(\"Average test error : \", np.mean(test_error_list))\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "print(\"\\n\\n\")\n",
        "print(\"================== Using PolynomialFeatures(degree=2) ======================\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "for i in np.logspace(1,7,num=13):\n",
        "  print(\"---------------------- \", i,\"----------------------\")\n",
        "  \n",
        "  test, train = k_fold_cross_validation_for_RR(13, X_2, y_2, i)\n",
        "  print(\"Test Error \", test, \"Train Error  \", train)\n",
        "  print(\"\\n\")\n",
        "\n",
        "# print(\"Average train error: \", np.mean(train_error_list))\n",
        "# print(\"Average test error : \", np.mean(test_error_list))\n",
        "print(\"----------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "===================== Error for Ridge Regression ===============================\n",
            "\n",
            "\n",
            "\n",
            "----------------------  10.0 ----------------------\n",
            "Average train error:  23.744830717361012\n",
            "Average test error :  21.909107606951896\n",
            "Test Error  20.257047684784677 Train Error   44.37520576905882\n",
            "\n",
            "\n",
            "----------------------  31.622776601683793 ----------------------\n",
            "Average train error:  24.032539044297817\n",
            "Average test error :  22.28971802780797\n",
            "Test Error  20.617757706422093 Train Error   45.70334308336826\n",
            "\n",
            "\n",
            "----------------------  100.0 ----------------------\n",
            "Average train error:  25.21802757161072\n",
            "Average test error :  23.693429811532578\n",
            "Test Error  21.94463034147355 Train Error   48.90915092711351\n",
            "\n",
            "\n",
            "----------------------  316.22776601683796 ----------------------\n",
            "Average train error:  29.21717426911817\n",
            "Average test error :  28.034013497430863\n",
            "Test Error  26.080609596701102 Train Error   56.170224587084654\n",
            "\n",
            "\n",
            "----------------------  1000.0 ----------------------\n",
            "Average train error:  39.102147587778646\n",
            "Average test error :  38.25704978696466\n",
            "Test Error  35.95904969821885 Train Error   70.91119682779149\n",
            "\n",
            "\n",
            "----------------------  3162.2776601683795 ----------------------\n",
            "Average train error:  54.259185893129555\n",
            "Average test error :  53.65893374434964\n",
            "Test Error  50.90825507024102 Train Error   92.37683361838496\n",
            "\n",
            "\n",
            "----------------------  10000.0 ----------------------\n",
            "Average train error:  69.48201000869003\n",
            "Average test error :  68.98857419054305\n",
            "Test Error  65.69747442986193 Train Error   113.06940405050086\n",
            "\n",
            "\n",
            "----------------------  31622.776601683792 ----------------------\n",
            "Average train error:  78.86556980387954\n",
            "Average test error :  78.40233370519329\n",
            "Test Error  74.74012428308912 Train Error   125.36939381048535\n",
            "\n",
            "\n",
            "----------------------  100000.0 ----------------------\n",
            "Average train error:  82.81324056177367\n",
            "Average test error :  82.35789551796651\n",
            "Test Error  78.5339543681798 Train Error   130.45427550765967\n",
            "\n",
            "\n",
            "----------------------  316227.7660168379 ----------------------\n",
            "Average train error:  84.19517548021028\n",
            "Average test error :  83.74207505895559\n",
            "Test Error  79.86090831852403 Train Error   132.22294757028826\n",
            "\n",
            "\n",
            "----------------------  1000000.0 ----------------------\n",
            "Average train error:  84.64706345345135\n",
            "Average test error :  84.19464432031246\n",
            "Test Error  80.29470299617077 Train Error   132.8000703742888\n",
            "\n",
            "\n",
            "----------------------  3162277.6601683795 ----------------------\n",
            "Average train error:  84.79150476019427\n",
            "Average test error :  84.33929809866653\n",
            "Test Error  80.43334944476769 Train Error   132.98441594650703\n",
            "\n",
            "\n",
            "----------------------  10000000.0 ----------------------\n",
            "Average train error:  84.8373370450855\n",
            "Average test error :  84.38519727208495\n",
            "Test Error  80.4773418247842 Train Error   133.04289747344424\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================== Using PolynomialFeatures(degree=2) ======================\n",
            "\n",
            "\n",
            "\n",
            "----------------------  10.0 ----------------------\n",
            "Average train error:  13.421591299161484\n",
            "Average test error :  10.044478093371858\n",
            "Test Error  9.203691183870086 Train Error   29.748698809980397\n",
            "\n",
            "\n",
            "----------------------  31.622776601683793 ----------------------\n",
            "Average train error:  15.782945504186541\n",
            "Average test error :  12.71756527165518\n",
            "Test Error  11.745339554605088 Train Error   32.97243743423306\n",
            "\n",
            "\n",
            "----------------------  100.0 ----------------------\n",
            "Average train error:  18.955330351290836\n",
            "Average test error :  16.18221075152217\n",
            "Test Error  14.897598098703217 Train Error   38.480142131387744\n",
            "\n",
            "\n",
            "----------------------  316.22776601683796 ----------------------\n",
            "Average train error:  22.02987254761437\n",
            "Average test error :  19.651158685028086\n",
            "Test Error  18.020607577187295 Train Error   43.69284721212134\n",
            "\n",
            "\n",
            "----------------------  1000.0 ----------------------\n",
            "Average train error:  26.042766508729507\n",
            "Average test error :  24.182033725149715\n",
            "Test Error  22.339955924648244 Train Error   49.926444032977265\n",
            "\n",
            "\n",
            "----------------------  3162.2776601683795 ----------------------\n",
            "Average train error:  34.220612552724425\n",
            "Average test error :  32.8586951798724\n",
            "Test Error  30.793687038781073 Train Error   62.282873547484904\n",
            "\n",
            "\n",
            "----------------------  10000.0 ----------------------\n",
            "Average train error:  47.39800006925729\n",
            "Average test error :  46.4556762310784\n",
            "Test Error  44.01481995847572 Train Error   81.90280760541827\n",
            "\n",
            "\n",
            "----------------------  31622.776601683792 ----------------------\n",
            "Average train error:  62.34305793042615\n",
            "Average test error :  61.7028555337704\n",
            "Test Error  58.74755831294276 Train Error   103.42326438505543\n",
            "\n",
            "\n",
            "----------------------  100000.0 ----------------------\n",
            "Average train error:  74.60111640719539\n",
            "Average test error :  74.09141537525541\n",
            "Test Error  70.63454766886565 Train Error   119.94488721887221\n",
            "\n",
            "\n",
            "----------------------  316227.7660168379 ----------------------\n",
            "Average train error:  81.07956588793725\n",
            "Average test error :  80.6102842344567\n",
            "Test Error  76.87026811461928 Train Error   128.29773204539956\n",
            "\n",
            "\n",
            "----------------------  1000000.0 ----------------------\n",
            "Average train error:  83.59762937518714\n",
            "Average test error :  83.14024588546073\n",
            "Test Error  79.28803979853778 Train Error   131.48516986046005\n",
            "\n",
            "\n",
            "----------------------  3162277.6601683795 ----------------------\n",
            "Average train error:  84.45274120244046\n",
            "Average test error :  83.99898201503295\n",
            "Test Error  80.10846162121013 Train Error   132.5607193105155\n",
            "\n",
            "\n",
            "----------------------  10000000.0 ----------------------\n",
            "Average train error:  84.7295037830248\n",
            "Average test error :  84.27687486367607\n",
            "Test Error  80.37393235009884 Train Error   132.90810615590763\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx1anSrZVEHe"
      },
      "source": [
        ""
      ],
      "execution_count": 195,
      "outputs": []
    }
  ]
}