{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqptk25Lb7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975a3fa4-15bc-48d0-87f4-7ca89d770efc"
      },
      "source": [
        "\n",
        "import sys\n",
        "import csv\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from numpy import *\n",
        "np.seterr(divide='ignore', invalid='ignore')"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gki9klZxLb7J"
      },
      "source": [
        "## Multiclass classifier decision tree using ID3 algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrk0pgOQLb7J"
      },
      "source": [
        "feature = []"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqLJ34esLb7J"
      },
      "source": [
        "#normalize the entire dataset prior to learning using min-max normalization \n",
        "def normalize(matrix):\n",
        "#     transfer the data metrix to np array in float type.\n",
        "    a = np.array(matrix)\n",
        "    a = a.astype(float)\n",
        "    \n",
        "    # print(\"normalizing the entire dataset:\")\n",
        "    # print(a)\n",
        "    # print(\"Before normalizing\")\n",
        "#     apply the normalization along the 0 axis of a using the formula: (x - x_min)/(x_max - x_min)\n",
        "    \n",
        "   \n",
        "    a_min = a.min(axis=0)\n",
        "    a_max = a.max(axis=0)\n",
        "    val = (a - a_min)/(a_max - a_min)\n",
        "    return val\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1KZ2mmHLb7J"
      },
      "source": [
        "# reading from the file using numpy genfromtxt with delimiter ','\n",
        "def load_csv(file):\n",
        "  X = np.genfromtxt(file, delimiter=\",\", dtype=str)\n",
        "  return (X)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keM_d5-nLb7J"
      },
      "source": [
        "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
        "def random_numpy_array(ar):\n",
        "  np.random.shuffle(ar)\n",
        "  arr = ar\n",
        "  return arr"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muQlwBQcLb7K"
      },
      "source": [
        "#Normalize the data and generate the training labels,training features, test labels and test training\n",
        "def generate_set(X):\n",
        "    print(X.shape[0])\n",
        "#     store the label X[:,-1] to Y\n",
        "    Y = X[:,-1]\n",
        "\n",
        "#     reshape y to (Y's length, 1)\n",
        "#     store it to j\n",
        "    j = Y.reshape((len(Y),1))\n",
        "\n",
        "    # print(\"J is\",j)\n",
        "#     create the new_X which exclude the label X[:,:-1]\n",
        "    new_X = X[:,:-1]\n",
        "\n",
        "#     normalize the data step\n",
        "#     using our implemented function normalize()\n",
        "    norm_X = normalize(new_X)\n",
        "\n",
        "#     add the label back to the normiazlied X\n",
        "#     using np.concatenate along axis=1\n",
        "    X_after_norm = np.concatenate((norm_X, j), axis=1)\n",
        "\n",
        "\n",
        "#     store the size of rows of the normalized X with labels\n",
        "    rows_size = X_after_norm.shape[0]\n",
        "\n",
        "\n",
        "#     use the 10% of the data to be the test set.\n",
        "#     store the number of testing data\n",
        "    num_test_data = round(0.1 * rows_size)\n",
        "\n",
        "#     set the starting idex to be 0\n",
        "    start = 0\n",
        "\n",
        "#     set the ending index to be the number of testing data\n",
        "    end = num_test_data\n",
        "\n",
        "#     create a list that store all features of the testing data\n",
        "    features_testData_list = []\n",
        "\n",
        "#     create a list that store all labels of the testing data\n",
        "    labels_testData_list = []\n",
        "\n",
        "#     create a list that store all features of the training data\n",
        "    features_trainData_list = []\n",
        "    \n",
        "#     create a list that store all labels of the training data\n",
        "    labels_trainData_list = []\n",
        "#     10-fold cross-validation:\n",
        "    for i in range(10):\n",
        "#         store the test set for corss-validation using X[start:end,:]\n",
        "      X_after_norm_test = X_after_norm[start:end,:]\n",
        "\n",
        "#         get training data before the testing data X[:start, :]\n",
        "      train_bef_test = X_after_norm[:start,:]\n",
        "\n",
        "#         get training data after the testing data X[end:, :]\n",
        "      train_aft_test = X_after_norm[end:,:]\n",
        "\n",
        "#         form the new training set using np.concatenate\n",
        "      X_after_norm_training = np.concatenate((train_bef_test,train_aft_test),axis=0)\n",
        "\n",
        "#         get the testing set labels\n",
        "      test_set_labels = X_after_norm_test[:,-1]\n",
        "\n",
        "#         flattent the labels\n",
        "      test_set_labels = test_set_labels.flatten()\n",
        "\n",
        "#         get the training set labels\n",
        "      train_set_labels = X_after_norm_training[:,-1]\n",
        "\n",
        "#         flattent the labels\n",
        "      train_set_labels = train_set_labels.flatten()\n",
        "\n",
        "#         create the test set exclude the labels\n",
        "      X_after_norm_test = X_after_norm_test[:,:-1]\n",
        "      X_after_norm_test = X_after_norm_test.astype(float)\n",
        "\n",
        "\n",
        "#         same for the training set\n",
        "      X_after_norm_training = X_after_norm_training[:,:-1]\n",
        "      X_after_norm_training = X_after_norm_training.astype(float)\n",
        "\n",
        "#         append test data of this fold to the list\n",
        "      features_testData_list.append(X_after_norm_test)\n",
        "\n",
        "\n",
        "#         append test lables of this fold to the list\n",
        "      labels_testData_list.append(test_set_labels)\n",
        "\n",
        "#         do the same for the training set\n",
        "      features_trainData_list.append(X_after_norm_training)\n",
        "      labels_trainData_list.append(train_set_labels)\n",
        "\n",
        "\n",
        "#         update the index pointer\n",
        "      start = end \n",
        "      end = end + num_test_data\n",
        "\n",
        "\n",
        "#     return the fold list that contain data and label for both training and testing set.\n",
        "    return features_testData_list, labels_testData_list, features_trainData_list, labels_trainData_list\n"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQl1HKbOLb7K"
      },
      "source": [
        "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
        "def build_dict_of_attributes_with_class_values(X,y):\n",
        "#     init a dict for attributes\n",
        "  attributes_dictionary = {}\n",
        "\n",
        "#     init feature list.\n",
        "  fetaure_list = []\n",
        "\n",
        "#     for each feature in the dataset\n",
        "  for i in range(X.shape[1]):\n",
        "#         store the featur index\n",
        "    feature_idx = i\n",
        "\n",
        "#     find all the value correspond to this feature\n",
        "    val = X[:,i]\n",
        "\n",
        "#     init an attribute list\n",
        "    attribute_list = []\n",
        "\n",
        "#     init the counter to 0\n",
        "    cnt = 0\n",
        "\n",
        "#         for each value in the \"all the value correspond to this feature\"\n",
        "    for element in val:\n",
        "#             init a empty list that store the attribute value\n",
        "      attribute_val = []\n",
        "\n",
        "#             append the this value to the list\n",
        "      attribute_val.append(element)\n",
        "\n",
        "#             append the label of this value to the list\n",
        "      attribute_val.append(y[cnt])\n",
        "\n",
        "#             append this list to the attribute list.\n",
        "      attribute_list.append(attribute_val)\n",
        "\n",
        "#             increase the counter\n",
        "      cnt = cnt + 1\n",
        "\n",
        "#         add this attribute list to the dict according to the feature index\n",
        "    attributes_dictionary[feature_idx] = attribute_list\n",
        "\n",
        "#         append the feature indx to the feature list.\n",
        "    fetaure_list.append(feature_idx)\n",
        "\n",
        "#     return the dict and feature list.\n",
        "  return attributes_dictionary, fetaure_list\n"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmQCxYyMjKcp"
      },
      "source": [
        "# Y = {'a':1, 'b':2}\n",
        "# if ( 'a' in Y):\n",
        "#   print(Y['a'])"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAl6Re4Lb7K"
      },
      "source": [
        "# Iterative Dichotomiser 3 entropy calculation\n",
        "def entropy(y):\n",
        "#     init a class frequence dict\n",
        "  class_frequency_dictionary = {}\n",
        "\n",
        "#     init the attribute entropy to 0\n",
        "  attribute_entropy = 0\n",
        "\n",
        "#     for each label in y:\n",
        "  for i in y:\n",
        "#         if this is label is already in the dict, we increase its feq\n",
        "    if (i in class_frequency_dictionary):\n",
        "      class_frequency_dictionary[i] +=1\n",
        "\n",
        "#          else, we set the freq to 1\n",
        "    else:\n",
        "      class_frequency_dictionary[i] = 1\n",
        "\n",
        "\n",
        "#     calculate the cumulate entropy using the formula.\n",
        "    for val in class_frequency_dictionary.values():\n",
        "      p = val/float(len(y))\n",
        "\n",
        "      attribute_entropy += ((-p) * math.log(p,2))\n",
        "\n",
        "\n",
        "  return attribute_entropy\n"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxnryh98Lb7K"
      },
      "source": [
        "#Class node and explanation is self explaination\n",
        "class Node(object):\n",
        "#     init the node with val,lchild,rchild,theta and leaf.\n",
        "    def __init__(self, val, lchild, rchild,theta,leaf):\n",
        "        self.root_value = val\n",
        "        self.root_left = lchild\n",
        "        self.root_right = rchild\n",
        "        self.theta = theta\n",
        "        self.leaf = leaf\n",
        "\n",
        "#     method to identify if the node is leaf\n",
        "    def is_leaf(self):\n",
        "        \n",
        "        return self.leaf\n",
        "\n",
        "#     method to return threshold value\n",
        "    def ret_thetha(self):\n",
        "        \n",
        "        return self.theta\n",
        "    \n",
        "#     method return root value\n",
        "    def ret_root_value(self):\n",
        "        \n",
        "        return self.root_value\n",
        "    \n",
        "#     method return left tree\n",
        "    def ret_llist(self):\n",
        "        \n",
        "        return self.root_left\n",
        "\n",
        "#     method return right tree\n",
        "    def ret_rlist(self):\n",
        "        \n",
        "        return self.root_right\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHv7XwzLb7K"
      },
      "source": [
        "#Decision tree object\n",
        "class DecisionTree(object):\n",
        "#     init a variable called fea_list\n",
        "  fea_list = []\n",
        "\n",
        "#     init the Dtree by setting the root node to None\n",
        "  def __init__(self):\n",
        "      self.root_node = None\n",
        "\n",
        "  #method to return the major class value using Counter() and .most_common()\n",
        "  def cal_major_class_values(self,class_values):\n",
        "    val = Counter(class_values)\n",
        "    val = val.most_common(1)[0][0]\n",
        "  \n",
        "    return val \n",
        "\n",
        "  #method to calculate best threshold value for each feature\n",
        "  def cal_best_theta_value(self,ke,attri_list):\n",
        "#         init a list for data\n",
        "    val = []\n",
        "\n",
        "#         init a list for class labes\n",
        "    class_values = []\n",
        "\n",
        "\n",
        "#         for each attribute in the attri_list\n",
        "    for i in attri_list:\n",
        "#             append the data\n",
        "      val.append(i[0])\n",
        "\n",
        "#             append the feature value.\n",
        "      class_values.append(i[1])\n",
        "\n",
        "#         calculate the entropy of those feaure values\n",
        "    entropy_feature_i = entropy(class_values)\n",
        "\n",
        "#         init the max info gain = 0\n",
        "    max_info_gain = 0\n",
        "\n",
        "#         init theta=0\n",
        "    theta = 0\n",
        "\n",
        "#         init a list that store the best index on the left\n",
        "    best_index_on_left_list = []\n",
        "\n",
        "#         init a list that store the best index on the right\n",
        "    best_index_on_right_list = []\n",
        "\n",
        "#         init a list that store class labels after split\n",
        "    class_labels_after_split_list = []\n",
        "\n",
        "\n",
        "#         sort the data\n",
        "    val.sort()\n",
        "\n",
        "#         for each index of data:\n",
        "    for idx in range(len(val)-1):\n",
        "\n",
        "\n",
        "#             calculate the current theta using data[i]+data[i+1])/ 2\n",
        "      current_theta = float(val[idx]+val[idx+1])/ 2\n",
        "\n",
        "#             init a list that store index that less than theta\n",
        "      index_less_than_theta_list = []\n",
        "\n",
        "#             init a list that store value that less than theta\n",
        "      values_less_than_theta_list = []\n",
        "\n",
        "#             init a list that store index that greater than theta\n",
        "      index_greater_than_theta_list = []\n",
        "\n",
        "#             init a list that store value that less than theta\n",
        "      values_greater_than_theta_list = []\n",
        "\n",
        "#             init the counter to 0\n",
        "      cnt = 0\n",
        "\n",
        "#             for each index and value in attri_list\n",
        "      for c,j in enumerate(attri_list):\n",
        "#                 if value less or equal than the current theta:\n",
        "        if (j[0] <= current_theta):\n",
        "\n",
        "\n",
        "#                     update the \"less\" list of index and value\n",
        "          index_less_than_theta_list.append(c)\n",
        "          values_less_than_theta_list.append(j[1])\n",
        "\n",
        "#                 else:\n",
        "        else:\n",
        "#                     update the \"greater\" list of index and value\n",
        "          index_greater_than_theta_list.append(c)\n",
        "          values_greater_than_theta_list.append(j[1])\n",
        "#             calculate the entropy of the \"less\" list\n",
        "      less_list_entropy = entropy(values_less_than_theta_list)\n",
        "#             calculate the entropy of the \"greater\" list\n",
        "      greater_list_entropy = entropy(values_greater_than_theta_list)\n",
        "#             calculate the info gain using the formular.\n",
        "\n",
        "      num1 = len(index_less_than_theta_list)\n",
        "      num2 = len(index_greater_than_theta_list)\n",
        "      num3 = len(attri_list)\n",
        "\n",
        "      current_info_gain = entropy_feature_i - (less_list_entropy * (num1/float(num3))) - (greater_list_entropy * (num2/float(num3)))\n",
        "#             if current info gain > max info gan\n",
        "      if ( current_info_gain > max_info_gain):\n",
        "        \n",
        "#                 update the info gain, \n",
        "        max_info_gain = current_info_gain\n",
        "#                     the theta, the best index list of right, \n",
        "        theta = current_theta\n",
        "#                         the best index list of left and class_labels_list_after_split\n",
        "        best_index_on_left_list = index_less_than_theta_list\n",
        "        best_index_on_right_list = index_greater_than_theta_list\n",
        "\n",
        "        class_labels_after_split_list = values_greater_than_theta_list + values_less_than_theta_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         return the max info gain, theata,the best left list,the best right list and class label after split\n",
        "    return max_info_gain, theta, best_index_on_left_list, best_index_on_right_list, class_labels_after_split_list\n",
        "\n",
        "#method to select the best feature out of all the features.\n",
        "  def best_feature(self,dict_rep):\n",
        "        \n",
        "#         set key value to none\n",
        "    key_value = None\n",
        "#         set best info gain to -1\n",
        "    best_info_gain = -1 \n",
        "\n",
        "#         set best theta to 0\n",
        "    best_theta = 0\n",
        "\n",
        "#         set best left list to empty\n",
        "    best_index_on_left_list = []\n",
        "\n",
        "#         set best right list to empty\n",
        "    best_index_on_right_list = []\n",
        "\n",
        "#         set best class labels after split to empty\n",
        "    best_class_labels_after_split_list = []\n",
        "\n",
        "#         init a result list\n",
        "    result_list = []\n",
        "\n",
        "\n",
        "#         for each key in dict_rep:\n",
        "    for ke in dict_rep.keys():\n",
        "#             using cal_best_theta_value() and store all returned values\n",
        "      info_gain, theta, index_on_left_list, index_on_right_list, class_labels_after_split_l = self.cal_best_theta_value(ke, dict_rep[ke])\n",
        "\n",
        "#             if info gian is greater than best info gain:\n",
        "      if (info_gain > best_info_gain):\n",
        "#                 update info gain, theth, key value,\n",
        "#                 left list, right list, class labels after split\n",
        "        best_info_gain = info_gain\n",
        "        best_theta = theta\n",
        "        key_value = ke\n",
        "        best_index_on_left_list = index_on_left_list\n",
        "        best_index_on_right_list = index_on_right_list\n",
        "        best_class_labels_after_split_list = class_labels_after_split_l\n",
        "\n",
        "#         append the key value to the retrun list\n",
        "    result_list.append(key_value)\n",
        "\n",
        "#         append the theta value to the retrun list\n",
        "    result_list.append(best_theta)\n",
        "\n",
        "#         append the left list to the retrun list\n",
        "    result_list.append(best_index_on_left_list)\n",
        "\n",
        "#         append the right list to the retrun list\n",
        "    result_list.append(best_index_on_right_list)\n",
        "\n",
        "#         append the class labels to the retrun list\n",
        "    result_list.append(best_class_labels_after_split_list)\n",
        "\n",
        "#         return the list.\n",
        "    return result_list\n",
        "\n",
        "  def get_remainder_dict(self,dict_of_everything,index_split):\n",
        "    global fea_list\n",
        "#         init a split dict\n",
        "    split_dict = {}\n",
        "\n",
        "#         for each key \"ke\" in dict_of_everything:\n",
        "    for ke in dict_of_everything.keys():\n",
        "#             init a value list\n",
        "      value_list = []\n",
        "\n",
        "#             init a modified list\n",
        "      modified_list = []\n",
        "\n",
        "#             get the corresponding values of the key\"ke\" \n",
        "      corresp_val_of_keys = dict_of_everything[ke]\n",
        "\n",
        "#             for each value and its corresponding index of the key\"ke\" \n",
        "      # for index, value in enumerate(corresp_val_of_keys):\n",
        "      for each_val in range(len(corresp_val_of_keys)):\n",
        "#                 if index is not in the index_split:\n",
        "        if ( each_val not in index_split):\n",
        "\n",
        "#                     append it to the modified list and value list\n",
        "          modified_list.append(corresp_val_of_keys[each_val])\n",
        "          value_list.append(corresp_val_of_keys[each_val][1]) # 1 ho ki i\n",
        "#             add this modified list to the dict\n",
        "      split_dict[ke] = modified_list\n",
        "      key_name = split_dict.keys()\n",
        "\n",
        "#         return the splited dict and val list\n",
        "    return split_dict, value_list\n",
        "\n",
        "  #method to create decision tree\n",
        "  def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
        "\n",
        "    global fea_list\n",
        "    #if all the class labels are same, then we are set\n",
        "    if (len(set(class_val)) ==1):\n",
        "      # print(\"Leaf node for set class is\",class_val[0],len(class_val))\n",
        "\n",
        "      # root_node = Node(majority_val, None, None, 0, True)\n",
        "      p = Node(class_val[0], None, None, 0, True)\n",
        "\n",
        "        \n",
        "      return p\n",
        "    #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
        "    elif (len(class_val) < eta_min_val):\n",
        "      majority_val = self.cal_major_class_values(class_val)\n",
        "      q = Node(majority_val, None, None, 0, True)\n",
        "      return q          \n",
        "\n",
        "    else:\n",
        "#             using the best_feature to get best feature list\n",
        "      best_features_list = self.best_feature(dict_of_everything)\n",
        "#             store the node name, theta,left split, right split and class labes\n",
        "      node_name = best_features_list[0]\n",
        "      theta = best_features_list[1]\n",
        "      left_split = best_features_list[2]\n",
        "      right_split = best_features_list[3]\n",
        "      class_labels = best_features_list[4]\n",
        "      # print(\"Hello\")\n",
        "\n",
        "#             call get_remainder_dict to get left tree data\n",
        "      left_dict, class_value_1 = self.get_remainder_dict(dict_of_everything, left_split)\n",
        "\n",
        "#             call get_remainder_dict to get right tree data\n",
        "      right_dict, class_value_2 = self.get_remainder_dict(dict_of_everything, right_split)\n",
        "\n",
        "#             call create_decision_tree to get left tree based on the left tree data\n",
        "      \n",
        "      leftChild = self.create_decision_tree(left_dict, class_value_1,eta_min_val)\n",
        "      # leftChild = None\n",
        "\n",
        "#             call create_decision_tree to get right tree based on therightleft tree data\n",
        "      rightChild = self.create_decision_tree(right_dict, class_value_2,eta_min_val)\n",
        "     \n",
        "      # print(\"LeftChild \", leftChild)\n",
        "      # print(\"rightChild \", rightChild)\n",
        "#             set the root node\n",
        "      root_node = Node(node_name, rightChild, leftChild, theta, False)\n",
        "      # root_node = Node(node_name, theta, False)\n",
        "      # print(\"root node \", root_node)\n",
        "\n",
        "\n",
        "#             return root node\n",
        "      return root_node\n",
        "        \n",
        "  #fit the decisin tree\n",
        "  def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
        "#         set the fea_list the value of features\n",
        "    global fea_list\n",
        "    fea_list = features   \n",
        "#         set the root node using the function create_decision_tree()\n",
        "    root_node = self.create_decision_tree(dict_of_everything, cl_val, eta_min_val)\n",
        "\n",
        "    return root_node\n",
        "\n",
        "  def classify(self,row,root):\n",
        "#         init the test dict\n",
        "    test_dict = {}\n",
        "\n",
        "#         add row to the dict\n",
        "    for k,j in enumerate(row):\n",
        "      test_dict[k] = j\n",
        "#         set the current node to root\n",
        "    current_node = root\n",
        "      \n",
        "\n",
        "#         while the current node is not leaf:\n",
        "    while not current_node.leaf:\n",
        "#             implement the case whether the current shoud go to the left\n",
        "      if (test_dict[current_node.root_value]) <= current_node.theta:\n",
        "        current_node = current_node.root_left\n",
        "\n",
        "#             implement the case whether the current shoud go to the right\n",
        "      else:\n",
        "        current_node = current_node.root_right\n",
        "\n",
        "    \n",
        "#         return the calss of the current node\n",
        "    return current_node.root_value\n",
        "      \n",
        "  #method to the labels for the test data\n",
        "  def predict(self, X, root):\n",
        "    predict_list = []\n",
        "    for data in X:\n",
        "#         predict using the classify()\n",
        "      predicted_val = self.classify(data,root)\n",
        "      predict_list.append(predicted_val)\n",
        "    return predict_list"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9nUrKMeLb7M"
      },
      "source": [
        "#calculating the predicited accuracy\n",
        "def accuracy_for_predicted_values(test_class_names1,l):\n",
        "#     init true and false count to 0\n",
        "  true_cnt = 0\n",
        "  false_cnt = 0\n",
        "\n",
        "#     for each prediction,if predict is correct then, true++ else, false++\n",
        "  for i in range(len(test_class_names1)):\n",
        "    if (test_class_names1[i]== l[i]):\n",
        "      true_cnt += 1\n",
        "      \n",
        "    else:\n",
        "      false_cnt += 1   \n",
        "      \n",
        "#     return the acc\n",
        "  return true_cnt, false_cnt, float(true_cnt)/len(l)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbrZqy3Lb7O"
      },
      "source": [
        "def main(num_arr, eta_min):\n",
        "\n",
        "  eta_min_val = round(eta_min*num_arr.shape[0])\n",
        "  #randomly shuffle the array so that we can divide the data into test/training\n",
        "  random_array_1 = random_numpy_array(num_arr)\n",
        "  \n",
        "  #divide data into test labels,test features,training labels, training features\n",
        "  features_testData_list, labels_testData_list, features_trainData_list, labels_trainData_list = generate_set(random_array_1)\n",
        "  \n",
        "#     init cumulate acc to 0\n",
        "  cumulative_acc = 0\n",
        "# standard deviation \n",
        "  standard_deviation_list = []\n",
        "  #ten fold iteration \n",
        "  for i in range(10):\n",
        "  \n",
        "    #build a dictionary with class labels and respective features values belonging to that class\n",
        "    attributes_dictionary, fetaure_list = build_dict_of_attributes_with_class_values(features_trainData_list[i], labels_trainData_list[i])\n",
        "    \n",
        "    #instantiate decision tree instance\n",
        "    decision_tree_dict = DecisionTree()\n",
        "    \n",
        "    # build the decision tree model.\n",
        "    decision_tree_model = decision_tree_dict.fit(attributes_dictionary,labels_trainData_list[i],fetaure_list,eta_min_val)\n",
        "    \n",
        "    #predict the class labels for test features\n",
        "    class_labels_pred = decision_tree_dict.predict(features_testData_list[i],decision_tree_model)\n",
        "    \n",
        "    #calculate the accuracy for the predicted values \n",
        "    right, wrong, accu = accuracy_for_predicted_values(labels_testData_list[i],class_labels_pred)   \n",
        "    \n",
        "#         add acc to cumulate acc\n",
        "    cumulative_acc += accu\n",
        "    standard_deviation_list.append(accu)\n",
        "\n",
        "    print(\"Accuracy is \",accu)\n",
        "  print(\"Accuracy across 10-cross validation for \",eta_min,\" is \",float(cumulative_acc)/10)\n",
        "  sum = 0\n",
        "  for e in standard_deviation_list:\n",
        "    sum = sum + (float(e) - float(cumulative_acc)/10)**2\n",
        "  print(\"Standard deviation across 10-cross validation for \",eta_min,\" is \", (sum/(10-1))**0.5)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWYMCHYLb7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4f55e0-e40f-49c5-fa93-dac3a474972b"
      },
      "source": [
        "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
        "# newfile = \"/spambase.csv\"\n",
        "file_name = input(\"Please press 1 for 'iris.csv' and 2 for 'spambase.csv': \")\n",
        "print(\"\\n\")\n",
        "if (file_name == '1'):\n",
        "  newfile = \"/iris.csv\"\n",
        "elif (file_name == '2'):\n",
        "  newfile = \"/spambase.csv\"\n",
        "\n",
        "else:\n",
        "  print(\"Wrong input!\")\n",
        "\n",
        "\n",
        "num_arr = load_csv(newfile)\n",
        "for i in eta_min_list:\n",
        "  main(num_arr,i)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please press 1 for 'iris.csv' and 2 for 'spambase.csv': 1\n",
            "\n",
            "\n",
            "150\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.8\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy across 10-cross validation for  0.05  is  0.9133333333333334\n",
            "Standard deviation across 10-cross validation for  0.05  is  0.06324555320336757\n",
            "\n",
            "\n",
            "150\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.8\n",
            "Accuracy is  1.0\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.7333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy across 10-cross validation for  0.1  is  0.9266666666666667\n",
            "Standard deviation across 10-cross validation for  0.1  is  0.09135468796041986\n",
            "\n",
            "\n",
            "150\n",
            "Accuracy is  1.0\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.8\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy across 10-cross validation for  0.15  is  0.9333333333333332\n",
            "Standard deviation across 10-cross validation for  0.15  is  0.07027283689263064\n",
            "\n",
            "\n",
            "150\n",
            "Accuracy is  1.0\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.8\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.8\n",
            "Accuracy across 10-cross validation for  0.2  is  0.9266666666666667\n",
            "Standard deviation across 10-cross validation for  0.2  is  0.0798145999825243\n",
            "\n",
            "\n",
            "150\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  1.0\n",
            "Accuracy is  1.0\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.8666666666666667\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy is  0.9333333333333333\n",
            "Accuracy across 10-cross validation for  0.25  is  0.9400000000000002\n",
            "Standard deviation across 10-cross validation for  0.25  is  0.049190985824841445\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeFVtb_QLb7P"
      },
      "source": [
        ""
      ],
      "execution_count": 210,
      "outputs": []
    }
  ]
}